# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Useful Resources
- [ScriptRunConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.scriptrunconfig?view=azure-ml-py)
- [Configure and submit training runs](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-set-up-training-targets)
- [HyperDriveConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py)
- [How to tune hyperparamters](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters)


## Summary

**This dataset includes information on people who have applied for bank loans. Here, our goal is to create a model that can estimate a person's likelihood of subscribing to a service based on the data provided about that person.**

**With 91.8% accuracy, a voting ensemble was shown to be the best performing model using HyperDrive configurations. However, the accuracy of AutoML models was comparable.**

## Scikit-learn Pipeline
**The provided data is obtained using the Scikit-learn pipeline from the supplied URL. Several data cleaning procedures are carried out after data download, including:**
- Removing NAs from the dataset.
- One-hot encoding job titles, contact, and education variables.
- Encoding a number of other categorical variables.
- Encoding months of the year.
- Encoding the target variable.

**The data is divided into a training set and a test set after it has been produced. As a compromise between assuring proper representation in the test data and offering enough data for model training, a test set size of 33% of all entries was chosen.**

**I specified the parameter sampler as such:**

```
ps = RandomParameterSampling(
    {
        '--C' : choice(0.001,0.01,0.1,1,10,20,50,100,200,500,1000),
        '--max_iter': choice(50,100,200,300)
    }
)
```

**I chose discrete values with _choice_ for both parameters, _C_ and _max_iter_.**
**One of the options for the sampler is RandomParameterSampling_, and I choose it since it is the fastest and permits early termination of low-performance runs.**

**Early stopping policy**
**By automatically terminating underperforming runs, an early stopping policy increases computational efficiency. I selected the _BanditPolicy_, which I defined as:**
```
policy = BanditPolicy(evaluation_interval=2, slack_factor=0.1)
```
**The _evaluation interval_ specifies how frequently the policy will be applied and is optional. One interval is equal to each time the training script logs the primary metric.**
**_slack_factor_ is the amount of latitude given in relation to the most effective training run. The slack is specified by this factor as a ratio.**
**Any run that does not meet the evaluation metric's slack factor or slack amount relative to the best-performing run will be stopped. This indicates that under this strategy, the best-performing runs will be carried out to completion, which is why I selected it.**

**Logistic regression is the categorization technique employed in this case. A fitted logistic function and a threshold are used in logistic regression.**
**I defined the following configuration for the HyperDrive run:**
```
hyperdrive_config = HyperDriveConfig(run_config=src,
                    hyperparameter_sampling=ps,
                    policy=policy,
                    primary_metric_name='Accuracy',
                    primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,
                    max_total_runs=16,
                    max_concurrent_runs=4)
```

**I also migrated the estimator to ScriptRunConfig:**
```
src = ScriptRunConfig(source_directory=".",
                      script='train.py',
                      compute_target=compute_target,
                      environment=sklearn_env)
```

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

**I defined the following configuration for the AutoML run:**

```
automl_config = AutoMLConfig(
    compute_target = compute_target,
    experiment_timeout_minutes=30,
    task='classification',
    primary_metric='accuracy',
    training_data=ds,
    label_column_name='y',
    enable_onnx_compatible_models=True,
    n_cross_validations=2)
```

_experiment_timeout_minutes=30_

**This is an exit criterion that establishes the number of minutes the experiment should run for. I utilised the bare minimum of 30 minutes to assist prevent experiment time out failures.**

_task='classification'_

This identifies the sort of experiment, in this case, classification.

_primary_metric='accuracy'_

I choose accuracy as my main metric.

_enable_onnx_compatible_models=True_

I decided to turn on ONNX-compatible model enforcement.

_n_cross_validations=2_

Based on the same number of folds, this option determines how many cross validations will be carried out (number of subsets). In my code, I used two folds for cross-validation because one could lead to overfit. As a result, the metrics are derived using the average of the two validation metrics.

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

**Comparison of the two models and their performance. Differences in accuracy & architecture - comments**


| HyperDrive Model | |
| :---: | :---: |
| id | HD_64aeea36-8cdc-47eb-a2e2-6f0615caa0ad_9 |
| Accuracy | 0.9182094081942337 |


| AutoML Model | |
| :---: | :---: |
| id | AutoML_cdf347c6-ef4b-4df3-8347-87ca894a4a4d_35 |
| Accuracy | 0.9137177541729894 |
| Algortithm | VotingEnsemble |

**Despite the HyperDrive model doing better in terms of precision, there is not much of a difference in the two models' accuracy.**

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

Prior to training, it might be beneficial to investigate additional feature engineering procedures. Additionally, many AutoML runs use a scaler before training and evaluating the model. This scaling does not truly aid the encoded data, hence selective scaling of continuous variables as opposed to universal scaling could be useful. Additionally, if AutoML were run for a longer period of time, it would probably find better models. It would also be instructive to investigate hyperdrive using a wider range of categorization methods.

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**
**I defined the following configuration for deleting the compute cluster**

```
compute_target.delete()
```
